{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Design"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Originally Contributed by**: Arpit Bhatia, Chris Coey"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial covers experiment design examples (D-optimal, A-optimal, and E-optimal) \nfrom section 7.5 of the book Convex Optimization by Boyd and Vandenberghe[[1]](#c1)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Design"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic experiment design problem is as follows. \nGiven the menu of possible choices for experiments, $v_{1}, \\ldots, v_{p}$, \nand the total number $m$ of experiments to be carried out, choose the numbers of each type of experiment, \n$i . e ., m_{1}, \\ldots, m_{p}$ to make the error covariance $E$ small (in some sense). \nThe variables $m_{1}, \\ldots, m_{p}$ must, of course, be integers and sum to $m,$ the given total number of experiments. \nThis leads to the optimization problem"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n\\begin{array}{cl}{\\operatorname{minimize}\\left(\\mathrm{w.r.t.} \\mathbf{S}_{+}^{n}\\right)} & {E=\\left(\\sum_{j=1}^{p} m_{j} v_{j} v_{j}^{T}\\right)^{-1}} \\\\ {\\text { subject to }} & {m_{i} \\geq 0, \\quad m_{1}+\\cdots+m_{p}=m} \\\\ {} & {m_{i} \\in \\mathbf{Z}}\\end{array}\n$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relaxed Experiment Design Problem"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic experiment design problem can be a hard combinatorial problem when $m,$ the total number of experiments, \nis comparable to $n$ , since in this case the $m_{i}$ are all small integers. \nIn the case when $m$ is large compared to $n$ , however, a good approximate solution can be found by ignoring, \nor relaxing, the constraint that the $m_{i}$ are integers. \nLet $\\lambda_{i}=m_{i} / m,$ which is the fraction of the total number of experiments for which \n$a_{j}=v_{i},$ or the relative frequency of experiment $i$. \nWe can express the error covariance in terms of $\\lambda_{i}$ as"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\nE=\\frac{1}{m}\\left(\\sum_{i=1}^{p} \\lambda_{i} v_{i} v_{i}^{T}\\right)^{-1}\n$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vector $\\lambda \\in \\mathbf{R}^{p}$ satisfies $\\lambda \\succeq 0, \n\\mathbf{1}^{T} \\lambda=1,$ and also, each $\\lambda_{i}$ is an integer multiple of $1 / m$. \nBy ignoring this last constraint, we arrive at the problem"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n\\begin{array}{ll}{\\operatorname{minimize}\\left(\\mathrm{w.r.t.} \\mathbf{S}_{+}^{n}\\right)} & {E=(1 / m)\\left(\\sum_{i=1}^{p} \\lambda_{i} v_{i} v_{i}^{T}\\right)^{-1}} \\\\ {\\text { subject to }} & {\\lambda \\succeq 0, \\quad \\mathbf{1}^{T} \\lambda=1}\\end{array}\n$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of Experiment Design Problems"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several scalarizations have been proposed for the experiment design problem, \nwhich is a vector optimization problem over the positive semidefinite cone."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using JuMP\nusing ECOS\nusing LinearAlgebra\n\nq = 4 # dimension of estimate space\np = 8 # number of experimental vectors\nn = 12 # \nnmax = 3 # upper bound on lambda\n\nfunction gen_V(q, p)\n    V = Array{Float64}(undef, q, p)\n    for i in 1:q, j in 1:p\n        v = randn()\n        if abs(v) < 1e-2\n            v = 0.\n        end\n        V[i, j] = v\n    end\n    return V\nend\n\nV = gen_V(q, p)\n\nwhile rank(V) != q\n    V = gen_V(q, p)\nend\n\neye = Matrix{Float64}(I, q, q)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A-optimal design"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In A-optimal experiment design, we minimize tr $E$, the trace of the covariance matrix. \nThis objective is simply the mean of the norm of the error squared:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n\\mathbf{E}\\|e\\|_{2}^{2}=\\mathbf{E} \\operatorname{tr}\\left(e e^{T}\\right)=\\operatorname{tr} E\n$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The A-optimal experiment design problem in SDP form is"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n\\begin{array}{ll}{\\operatorname{minimize}} & {\\mathbf{1}^{T} u} \\\\ {\\text { subject to }} & {\\left[\\begin{array}{cc}{\\sum_{i=1}^{p} \\lambda_{i} v_{i} v_{i}^{T}} & {e_{k}} \\\\ {e_{k}^{T}} & {u_{k}}\\end{array}\\right] \\succeq 0, \\quad k=1, \\ldots, n} \\\\ {} & {\\lambda \\succeq 0, \\quad \\mathbf{1}^{T} \\lambda=1}\\end{array}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#aOpt = Model(with_optimizer(ECOS.Optimizer, printlevel = 0))\naOpt = Model()\n@variable(aOpt, np[1:p], Int, lower_bound = 0, upper_bound = nmax)\n@variable(aOpt, u[1:q], lower_bound = 0)\n\n@constraint(aOpt, sum(np) <= n)\nfor i = 1:q\n    @SDconstraint(aOpt, [V * diagm(0 => np ./ n) * V' eye[:, i]; eye[i, :]' u[i]] >= 0)\nend\n\n@objective(aOpt, Min, sum(u))\n\noptimize!(aOpt)\n\n@show objective_value(aOpt);\n@show value.(np);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E-optimal design"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In $E$ -optimal design, we minimize the norm of the error covariance matrix, i.e. the maximum eigenvalue of $E$. \nSince the diameter (twice the longest semi-axis) of the confidence ellipsoid $\\mathcal{E}$ \nis proportional to $\\|E\\|_{2}^{1 / 2}$, \nminimizing $\\|E\\|_{2}$ can be interpreted geometrically as minimizing the diameter of the confidence ellipsoid. \nE-optimal design can also be interpreted as minimizing the maximum variance of $q^{T} e$,\nover all $q$ with $\\|q\\|_{2}=1$. \nThe E-optimal experiment design problem in SDP form is"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n\\begin{array}{cl}{\\operatorname{maximize}} & {t} \\\\ {\\text { subject to }} & {\\sum_{i=1}^{p} \\lambda_{i} v_{i} v_{i}^{T} \\succeq t I} \\\\ {} & {\\lambda \\succeq 0, \\quad \\mathbf{1}^{T} \\lambda=1}\\end{array}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#eOpt = Model(with_optimizer(ECOS.Optimizer, printlevel = 0))\neOpt = Model()\n@variable(eOpt, np[1:p], Int, lower_bound = 0, upper_bound = nmax)\n@variable(eOpt, t)\n\n@SDconstraint(eOpt, V * diagm(0 => np ./ n) * V' - (t .* eye) >= 0)\n@constraint(eOpt, sum(np) <= n)\n\n@objective(eOpt, Max, t)\n\noptimize!(eOpt)\n\n@show objective_value(eOpt);\n@show value.(np);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D-optimal design\nThe most widely used scalarization is called $D$ -optimal design, \nin which we minimize the determinant of the error covariance matrix $E$. \nThis corresponds to designing the experiment to minimize the volume of the resulting confidence ellipsoid \n(for a fixed confidence level). \nIgnoring the constant factor 1$/ m$ in $E$, and taking the logarithm of the objective, \nwe can pose this problem as convex optimization problem"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n\\begin{array}{ll}{\\operatorname{minimize}} & {\\log \\operatorname{det}\\left(\\sum_{i=1}^{p} \\lambda_{i} v_{i} v_{i}^{T}\\right)^{-1}} \\\\ {\\text { subject to }} & {\\lambda \\succeq 0, \\quad \\mathbf{1}^{T} \\lambda=1}\\end{array}\n$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dOpt = Model()\n@variable(dOpt, np[1:p], Int, lower_bound = 0, upper_bound = nmax)\n@variable(dOpt, t)\n@objective(dOpt, Max, t)\n@constraint(dOpt, sum(np) <= n)\n@constraint(dOpt, [t, 1, V * diagm(0 => np ./ n) * V'] in MOI.LogDetConeSquare(q))\n\noptimize!(dOpt)\n\n@show objective_value(dOpt);\n@show value.(np);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n<a id='c1'></a>\n1. Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511804441"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.0.3"
    },
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
